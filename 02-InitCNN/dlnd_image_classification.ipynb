{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f6ee00be80>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.array(x/255)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "encoder = preprocessing.LabelBinarizer()\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    encoder.fit(x)\n",
    "    encoder.classes_ = np.array(list(range(10)))\n",
    "    return encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=[None, *image_shape], name = 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name = 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_depth = int(x_tensor.get_shape()[3])\n",
    "#     the typecast to int is needed because in weights->truncated the expected input_depth is int\n",
    "    \n",
    "    conv_ksize_height = conv_ksize[0]\n",
    "    conv_ksize_width = conv_ksize[1]\n",
    "#     What do kernel height and width represent? How do we know that conv_ksize[0] is height? \n",
    "    \n",
    "    conv_stride_height = conv_strides[0]\n",
    "    conv_stride_width = conv_strides[1]\n",
    "    \n",
    "#     Similar attributes for the maxpool layer\n",
    "    pool_ksize_height = pool_ksize[0]\n",
    "    pool_ksize_width = pool_ksize[1]\n",
    "    \n",
    "    pool_stride_height = pool_strides[0]\n",
    "    pool_stride_width = pool_strides[1]\n",
    "    \n",
    "    weights_shape = [conv_ksize_height, conv_ksize_width, input_depth, conv_num_outputs]\n",
    "    truncated = tf.truncated_normal(weights_shape, mean = 0.0, stddev = 0.05, dtype = tf.float32)\n",
    "    weights = tf.Variable(truncated)\n",
    "    biases = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv_strides = [1, conv_stride_height, conv_stride_width, 1]\n",
    "    \n",
    "    layer = tf.nn.conv2d(input = x_tensor, filter = weights, strides = conv_strides, padding = 'SAME')\n",
    "    layer = tf.nn.bias_add(layer, biases)\n",
    "    layer = tf.nn.relu(layer)\n",
    "#   non-linear activation needed\n",
    "    pool_shape = [1, pool_ksize_height, pool_ksize_width, 1]\n",
    "    pool_strides = [1, pool_stride_height, pool_stride_width, 1]\n",
    "    \n",
    "    layer = tf.nn.max_pool(layer, pool_shape, pool_strides, padding = 'SAME')\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    shape = x_tensor.get_shape().as_list() # returns shape = [None, 10, 30, 6]\n",
    "#     shape = list(x_tensor.get_shape()) # returns shape = [Dimension(None), Dimension(10), Dimension(30), Dimension(6)]\n",
    "#     print('shape',shape)\n",
    "    batch_sz = shape[0] or -1\n",
    "    height = shape[1]\n",
    "    width = shape[2]\n",
    "    depth = shape[3]\n",
    "    return tf.reshape(x_tensor, [batch_sz, height*width*depth])\n",
    "#     return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def connected(x_tensor, num_outputs):\n",
    "    '''Support function for fully_conn and output functions below\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs\n",
    "    - does not apply any activation\n",
    "    '''\n",
    "    batch_sz = x_tensor.get_shape().as_list()[1]\n",
    "    weights = tf.Variable(tf.truncated_normal((batch_sz, num_outputs), mean=0.0, stddev=0.05))\n",
    "#     how to choose the stddev above? How does contrib.layer choose it? What is the default in contrib.layer? \n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    connected_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    return connected_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    fully_conn_layer = connected(x_tensor, num_outputs)\n",
    "    fully_conn_layer = tf.nn.relu(fully_conn_layer)\n",
    "    return fully_conn_layer\n",
    "#     return tf.nn.relu(tf.contrib.layers.fully_connected(x_tensor, num_outputs))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    return connected(x_tensor, num_outputs)\n",
    "#     return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 18\n",
    "    conv_ksize = (4,4)\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (4,4)\n",
    "    pool_strides = (1,1)\n",
    "    num_outputs = 10 # for the 10 classes \n",
    "#   TK: try more conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides\n",
    "    \n",
    "    network = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    network = tf.nn.dropout(network, keep_prob)\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    network = flatten(network)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    network = fully_conn(network, 384)\n",
    "    network = tf.nn.dropout(network, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    output_network = output(network, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return output_network\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "#      no return here, this is an execution function\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : x: feature_batch: Batch of Numpy image data\n",
    "    : y: label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    : keep_prob: 1.0 # added by me\n",
    "    \"\"\"\n",
    "    acc = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    print('Acc: {} Loss: {}'.format(acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 75\n",
    "batch_size = 512\n",
    "keep_probability = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Acc: 0.2070000171661377 Loss: 2.2606866359710693\n",
      "Epoch  2, CIFAR-10 Batch 1:  Acc: 0.24139997363090515 Loss: 2.1199159622192383\n",
      "Epoch  3, CIFAR-10 Batch 1:  Acc: 0.3369999825954437 Loss: 1.976081132888794\n",
      "Epoch  4, CIFAR-10 Batch 1:  Acc: 0.378199964761734 Loss: 1.832689881324768\n",
      "Epoch  5, CIFAR-10 Batch 1:  Acc: 0.4275999665260315 Loss: 1.679229497909546\n",
      "Epoch  6, CIFAR-10 Batch 1:  Acc: 0.4562000036239624 Loss: 1.6051061153411865\n",
      "Epoch  7, CIFAR-10 Batch 1:  Acc: 0.47099992632865906 Loss: 1.5333713293075562\n",
      "Epoch  8, CIFAR-10 Batch 1:  Acc: 0.49379992485046387 Loss: 1.4611042737960815\n",
      "Epoch  9, CIFAR-10 Batch 1:  Acc: 0.5051999688148499 Loss: 1.4166672229766846\n",
      "Epoch 10, CIFAR-10 Batch 1:  Acc: 0.5061999559402466 Loss: 1.368802547454834\n",
      "Epoch 11, CIFAR-10 Batch 1:  Acc: 0.514799952507019 Loss: 1.320952296257019\n",
      "Epoch 12, CIFAR-10 Batch 1:  Acc: 0.5229998826980591 Loss: 1.27889084815979\n",
      "Epoch 13, CIFAR-10 Batch 1:  Acc: 0.5263999104499817 Loss: 1.2394481897354126\n",
      "Epoch 14, CIFAR-10 Batch 1:  Acc: 0.525399923324585 Loss: 1.196798324584961\n",
      "Epoch 15, CIFAR-10 Batch 1:  Acc: 0.5333999395370483 Loss: 1.1623128652572632\n",
      "Epoch 16, CIFAR-10 Batch 1:  Acc: 0.5433999300003052 Loss: 1.1183048486709595\n",
      "Epoch 17, CIFAR-10 Batch 1:  Acc: 0.5461999773979187 Loss: 1.083551049232483\n",
      "Epoch 18, CIFAR-10 Batch 1:  Acc: 0.5535999536514282 Loss: 1.0472323894500732\n",
      "Epoch 19, CIFAR-10 Batch 1:  Acc: 0.554599940776825 Loss: 1.034951090812683\n",
      "Epoch 20, CIFAR-10 Batch 1:  Acc: 0.5619999766349792 Loss: 0.9933929443359375\n",
      "Epoch 21, CIFAR-10 Batch 1:  Acc: 0.5637999176979065 Loss: 0.9780582785606384\n",
      "Epoch 22, CIFAR-10 Batch 1:  Acc: 0.5673999786376953 Loss: 0.91942298412323\n",
      "Epoch 23, CIFAR-10 Batch 1:  Acc: 0.5663999319076538 Loss: 0.9002440571784973\n",
      "Epoch 24, CIFAR-10 Batch 1:  Acc: 0.5681999325752258 Loss: 0.8920129537582397\n",
      "Epoch 25, CIFAR-10 Batch 1:  Acc: 0.5677999258041382 Loss: 0.8581892848014832\n",
      "Epoch 26, CIFAR-10 Batch 1:  Acc: 0.5693999528884888 Loss: 0.8330419659614563\n",
      "Epoch 27, CIFAR-10 Batch 1:  Acc: 0.574199914932251 Loss: 0.8194858431816101\n",
      "Epoch 28, CIFAR-10 Batch 1:  Acc: 0.5769999027252197 Loss: 0.799200713634491\n",
      "Epoch 29, CIFAR-10 Batch 1:  Acc: 0.5739998817443848 Loss: 0.7771028876304626\n",
      "Epoch 30, CIFAR-10 Batch 1:  Acc: 0.5813999176025391 Loss: 0.7738284468650818\n",
      "Epoch 31, CIFAR-10 Batch 1:  Acc: 0.5841999053955078 Loss: 0.7235175371170044\n",
      "Epoch 32, CIFAR-10 Batch 1:  Acc: 0.5827999711036682 Loss: 0.718305766582489\n",
      "Epoch 33, CIFAR-10 Batch 1:  Acc: 0.5913999080657959 Loss: 0.6837224364280701\n",
      "Epoch 34, CIFAR-10 Batch 1:  Acc: 0.5917999148368835 Loss: 0.6706568002700806\n",
      "Epoch 35, CIFAR-10 Batch 1:  Acc: 0.578999936580658 Loss: 0.701298713684082\n",
      "Epoch 36, CIFAR-10 Batch 1:  Acc: 0.5951999425888062 Loss: 0.6310335993766785\n",
      "Epoch 37, CIFAR-10 Batch 1:  Acc: 0.5889999270439148 Loss: 0.6287098526954651\n",
      "Epoch 38, CIFAR-10 Batch 1:  Acc: 0.5977998971939087 Loss: 0.6033364534378052\n",
      "Epoch 39, CIFAR-10 Batch 1:  Acc: 0.5961998701095581 Loss: 0.6043642163276672\n",
      "Epoch 40, CIFAR-10 Batch 1:  Acc: 0.5949999690055847 Loss: 0.5931424498558044\n",
      "Epoch 41, CIFAR-10 Batch 1:  Acc: 0.5969998836517334 Loss: 0.5841193795204163\n",
      "Epoch 42, CIFAR-10 Batch 1:  Acc: 0.600399911403656 Loss: 0.5548740029335022\n",
      "Epoch 43, CIFAR-10 Batch 1:  Acc: 0.6045998930931091 Loss: 0.5190531015396118\n",
      "Epoch 44, CIFAR-10 Batch 1:  Acc: 0.6019998788833618 Loss: 0.534369945526123\n",
      "Epoch 45, CIFAR-10 Batch 1:  Acc: 0.601599931716919 Loss: 0.5098670125007629\n",
      "Epoch 46, CIFAR-10 Batch 1:  Acc: 0.6019999384880066 Loss: 0.5128952860832214\n",
      "Epoch 47, CIFAR-10 Batch 1:  Acc: 0.6055999398231506 Loss: 0.48687830567359924\n",
      "Epoch 48, CIFAR-10 Batch 1:  Acc: 0.6019999384880066 Loss: 0.4740486443042755\n",
      "Epoch 49, CIFAR-10 Batch 1:  Acc: 0.6095998883247375 Loss: 0.4642544686794281\n",
      "Epoch 50, CIFAR-10 Batch 1:  Acc: 0.6101998686790466 Loss: 0.4426875114440918\n",
      "Epoch 51, CIFAR-10 Batch 1:  Acc: 0.6011999249458313 Loss: 0.45465409755706787\n",
      "Epoch 52, CIFAR-10 Batch 1:  Acc: 0.6155998706817627 Loss: 0.4253270626068115\n",
      "Epoch 53, CIFAR-10 Batch 1:  Acc: 0.6149998903274536 Loss: 0.40973514318466187\n",
      "Epoch 54, CIFAR-10 Batch 1:  Acc: 0.614599883556366 Loss: 0.40877988934516907\n",
      "Epoch 55, CIFAR-10 Batch 1:  Acc: 0.6145999431610107 Loss: 0.40588289499282837\n",
      "Epoch 56, CIFAR-10 Batch 1:  Acc: 0.610599935054779 Loss: 0.3924029469490051\n",
      "Epoch 57, CIFAR-10 Batch 1:  Acc: 0.6069998741149902 Loss: 0.3949475884437561\n",
      "Epoch 58, CIFAR-10 Batch 1:  Acc: 0.6089999079704285 Loss: 0.3694089949131012\n",
      "Epoch 59, CIFAR-10 Batch 1:  Acc: 0.6205999255180359 Loss: 0.3537289500236511\n",
      "Epoch 60, CIFAR-10 Batch 1:  Acc: 0.6063998937606812 Loss: 0.3654311001300812\n",
      "Epoch 61, CIFAR-10 Batch 1:  Acc: 0.6173999309539795 Loss: 0.3289925754070282\n",
      "Epoch 62, CIFAR-10 Batch 1:  Acc: 0.6061999201774597 Loss: 0.3414945602416992\n",
      "Epoch 63, CIFAR-10 Batch 1:  Acc: 0.6205998659133911 Loss: 0.3291623592376709\n",
      "Epoch 64, CIFAR-10 Batch 1:  Acc: 0.6063998937606812 Loss: 0.34183940291404724\n",
      "Epoch 65, CIFAR-10 Batch 1:  Acc: 0.6147999167442322 Loss: 0.306556761264801\n",
      "Epoch 66, CIFAR-10 Batch 1:  Acc: 0.6141999363899231 Loss: 0.3003336489200592\n",
      "Epoch 67, CIFAR-10 Batch 1:  Acc: 0.6139998435974121 Loss: 0.30850690603256226\n",
      "Epoch 68, CIFAR-10 Batch 1:  Acc: 0.6153998970985413 Loss: 0.3111613392829895\n",
      "Epoch 69, CIFAR-10 Batch 1:  Acc: 0.6123999357223511 Loss: 0.2917592227458954\n",
      "Epoch 70, CIFAR-10 Batch 1:  Acc: 0.6175999045372009 Loss: 0.2961059510707855\n",
      "Epoch 71, CIFAR-10 Batch 1:  Acc: 0.6041998863220215 Loss: 0.2835111618041992\n",
      "Epoch 72, CIFAR-10 Batch 1:  Acc: 0.6101999282836914 Loss: 0.27770352363586426\n",
      "Epoch 73, CIFAR-10 Batch 1:  Acc: 0.6131999492645264 Loss: 0.26806098222732544\n",
      "Epoch 74, CIFAR-10 Batch 1:  Acc: 0.6123999357223511 Loss: 0.2530602514743805\n",
      "Epoch 75, CIFAR-10 Batch 1:  Acc: 0.6151999235153198 Loss: 0.2624821662902832\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Acc: 0.141199991106987 Loss: 2.292141914367676\n",
      "Epoch  1, CIFAR-10 Batch 2:  Acc: 0.2619999647140503 Loss: 2.220061779022217\n",
      "Epoch  1, CIFAR-10 Batch 3:  Acc: 0.35359999537467957 Loss: 1.8594279289245605\n",
      "Epoch  1, CIFAR-10 Batch 4:  Acc: 0.40799999237060547 Loss: 1.6762678623199463\n",
      "Epoch  1, CIFAR-10 Batch 5:  Acc: 0.4285999834537506 Loss: 1.645232081413269\n",
      "Epoch  2, CIFAR-10 Batch 1:  Acc: 0.4501999616622925 Loss: 1.6584151983261108\n",
      "Epoch  2, CIFAR-10 Batch 2:  Acc: 0.4761999547481537 Loss: 1.4891963005065918\n",
      "Epoch  2, CIFAR-10 Batch 3:  Acc: 0.48259997367858887 Loss: 1.3551785945892334\n",
      "Epoch  2, CIFAR-10 Batch 4:  Acc: 0.4957999587059021 Loss: 1.3686282634735107\n",
      "Epoch  2, CIFAR-10 Batch 5:  Acc: 0.5027999877929688 Loss: 1.3785433769226074\n",
      "Epoch  3, CIFAR-10 Batch 1:  Acc: 0.5179999470710754 Loss: 1.4455411434173584\n",
      "Epoch  3, CIFAR-10 Batch 2:  Acc: 0.5281999111175537 Loss: 1.314115047454834\n",
      "Epoch  3, CIFAR-10 Batch 3:  Acc: 0.5217999219894409 Loss: 1.2136805057525635\n",
      "Epoch  3, CIFAR-10 Batch 4:  Acc: 0.538599967956543 Loss: 1.2235019207000732\n",
      "Epoch  3, CIFAR-10 Batch 5:  Acc: 0.540399968624115 Loss: 1.2697416543960571\n",
      "Epoch  4, CIFAR-10 Batch 1:  Acc: 0.5397999286651611 Loss: 1.322235107421875\n",
      "Epoch  4, CIFAR-10 Batch 2:  Acc: 0.5523999333381653 Loss: 1.233292818069458\n",
      "Epoch  4, CIFAR-10 Batch 3:  Acc: 0.5403999090194702 Loss: 1.1848864555358887\n",
      "Epoch  4, CIFAR-10 Batch 4:  Acc: 0.5641999244689941 Loss: 1.1589192152023315\n",
      "Epoch  4, CIFAR-10 Batch 5:  Acc: 0.5669999122619629 Loss: 1.1920369863510132\n",
      "Epoch  5, CIFAR-10 Batch 1:  Acc: 0.5645999312400818 Loss: 1.2458763122558594\n",
      "Epoch  5, CIFAR-10 Batch 2:  Acc: 0.5739999413490295 Loss: 1.1478676795959473\n",
      "Epoch  5, CIFAR-10 Batch 3:  Acc: 0.5699999332427979 Loss: 1.0948446989059448\n",
      "Epoch  5, CIFAR-10 Batch 4:  Acc: 0.5837998390197754 Loss: 1.0703402757644653\n",
      "Epoch  5, CIFAR-10 Batch 5:  Acc: 0.5789998769760132 Loss: 1.1449276208877563\n",
      "Epoch  6, CIFAR-10 Batch 1:  Acc: 0.5813999176025391 Loss: 1.1949539184570312\n",
      "Epoch  6, CIFAR-10 Batch 2:  Acc: 0.5743999481201172 Loss: 1.1382073163986206\n",
      "Epoch  6, CIFAR-10 Batch 3:  Acc: 0.587399959564209 Loss: 1.041297197341919\n",
      "Epoch  6, CIFAR-10 Batch 4:  Acc: 0.5925999283790588 Loss: 1.0312598943710327\n",
      "Epoch  6, CIFAR-10 Batch 5:  Acc: 0.5947998762130737 Loss: 1.094668984413147\n",
      "Epoch  7, CIFAR-10 Batch 1:  Acc: 0.595599889755249 Loss: 1.1366487741470337\n",
      "Epoch  7, CIFAR-10 Batch 2:  Acc: 0.5863999128341675 Loss: 1.093252182006836\n",
      "Epoch  7, CIFAR-10 Batch 3:  Acc: 0.5933999419212341 Loss: 0.9995617866516113\n",
      "Epoch  7, CIFAR-10 Batch 4:  Acc: 0.6061999201774597 Loss: 0.9800075888633728\n",
      "Epoch  7, CIFAR-10 Batch 5:  Acc: 0.5983999371528625 Loss: 1.0575982332229614\n",
      "Epoch  8, CIFAR-10 Batch 1:  Acc: 0.6059999465942383 Loss: 1.0891050100326538\n",
      "Epoch  8, CIFAR-10 Batch 2:  Acc: 0.6013998985290527 Loss: 1.055504560470581\n",
      "Epoch  8, CIFAR-10 Batch 3:  Acc: 0.6091998815536499 Loss: 0.9518459439277649\n",
      "Epoch  8, CIFAR-10 Batch 4:  Acc: 0.6163999438285828 Loss: 0.961550772190094\n",
      "Epoch  8, CIFAR-10 Batch 5:  Acc: 0.6075999140739441 Loss: 1.0070090293884277\n",
      "Epoch  9, CIFAR-10 Batch 1:  Acc: 0.6117998957633972 Loss: 1.0606740713119507\n",
      "Epoch  9, CIFAR-10 Batch 2:  Acc: 0.6031998991966248 Loss: 1.0330379009246826\n",
      "Epoch  9, CIFAR-10 Batch 3:  Acc: 0.619399905204773 Loss: 0.9006326198577881\n",
      "Epoch  9, CIFAR-10 Batch 4:  Acc: 0.6219999194145203 Loss: 0.9324452877044678\n",
      "Epoch  9, CIFAR-10 Batch 5:  Acc: 0.6155998706817627 Loss: 0.9624276757240295\n",
      "Epoch 10, CIFAR-10 Batch 1:  Acc: 0.6237998604774475 Loss: 1.0272901058197021\n",
      "Epoch 10, CIFAR-10 Batch 2:  Acc: 0.6145999431610107 Loss: 1.0132626295089722\n",
      "Epoch 10, CIFAR-10 Batch 3:  Acc: 0.6221998929977417 Loss: 0.8836092352867126\n",
      "Epoch 10, CIFAR-10 Batch 4:  Acc: 0.6261999011039734 Loss: 0.9241086840629578\n",
      "Epoch 10, CIFAR-10 Batch 5:  Acc: 0.6269999146461487 Loss: 0.9597398638725281\n",
      "Epoch 11, CIFAR-10 Batch 1:  Acc: 0.6289998888969421 Loss: 1.0001157522201538\n",
      "Epoch 11, CIFAR-10 Batch 2:  Acc: 0.6251999139785767 Loss: 0.9589055180549622\n",
      "Epoch 11, CIFAR-10 Batch 3:  Acc: 0.6235998868942261 Loss: 0.8613937497138977\n",
      "Epoch 11, CIFAR-10 Batch 4:  Acc: 0.6291999220848083 Loss: 0.8887715339660645\n",
      "Epoch 11, CIFAR-10 Batch 5:  Acc: 0.6239999532699585 Loss: 0.9232635498046875\n",
      "Epoch 12, CIFAR-10 Batch 1:  Acc: 0.6277998685836792 Loss: 0.9982369542121887\n",
      "Epoch 12, CIFAR-10 Batch 2:  Acc: 0.6237998604774475 Loss: 0.9433609247207642\n",
      "Epoch 12, CIFAR-10 Batch 3:  Acc: 0.6307999491691589 Loss: 0.8411661982536316\n",
      "Epoch 12, CIFAR-10 Batch 4:  Acc: 0.6389998197555542 Loss: 0.8624340295791626\n",
      "Epoch 12, CIFAR-10 Batch 5:  Acc: 0.6343998908996582 Loss: 0.8932749032974243\n",
      "Epoch 13, CIFAR-10 Batch 1:  Acc: 0.6317999362945557 Loss: 0.9624526500701904\n",
      "Epoch 13, CIFAR-10 Batch 2:  Acc: 0.6375998854637146 Loss: 0.8805538415908813\n",
      "Epoch 13, CIFAR-10 Batch 3:  Acc: 0.6387999057769775 Loss: 0.7895940542221069\n",
      "Epoch 13, CIFAR-10 Batch 4:  Acc: 0.6451998353004456 Loss: 0.8443084359169006\n",
      "Epoch 13, CIFAR-10 Batch 5:  Acc: 0.6351998448371887 Loss: 0.854547381401062\n",
      "Epoch 14, CIFAR-10 Batch 1:  Acc: 0.6427998542785645 Loss: 0.9411112070083618\n",
      "Epoch 14, CIFAR-10 Batch 2:  Acc: 0.6361998915672302 Loss: 0.8920583724975586\n",
      "Epoch 14, CIFAR-10 Batch 3:  Acc: 0.6423999071121216 Loss: 0.761746346950531\n",
      "Epoch 14, CIFAR-10 Batch 4:  Acc: 0.6451999545097351 Loss: 0.824722170829773\n",
      "Epoch 14, CIFAR-10 Batch 5:  Acc: 0.6537998914718628 Loss: 0.8383753895759583\n",
      "Epoch 15, CIFAR-10 Batch 1:  Acc: 0.6425999402999878 Loss: 0.9044048190116882\n",
      "Epoch 15, CIFAR-10 Batch 2:  Acc: 0.6475999355316162 Loss: 0.8591500520706177\n",
      "Epoch 15, CIFAR-10 Batch 3:  Acc: 0.6507999300956726 Loss: 0.7499629259109497\n",
      "Epoch 15, CIFAR-10 Batch 4:  Acc: 0.6541998386383057 Loss: 0.8009517788887024\n",
      "Epoch 15, CIFAR-10 Batch 5:  Acc: 0.6461999416351318 Loss: 0.795843243598938\n",
      "Epoch 16, CIFAR-10 Batch 1:  Acc: 0.6453999280929565 Loss: 0.8844353556632996\n",
      "Epoch 16, CIFAR-10 Batch 2:  Acc: 0.6505998969078064 Loss: 0.8565227389335632\n",
      "Epoch 16, CIFAR-10 Batch 3:  Acc: 0.6531999111175537 Loss: 0.718188464641571\n",
      "Epoch 16, CIFAR-10 Batch 4:  Acc: 0.6519998908042908 Loss: 0.7932668924331665\n",
      "Epoch 16, CIFAR-10 Batch 5:  Acc: 0.6527999043464661 Loss: 0.773852527141571\n",
      "Epoch 17, CIFAR-10 Batch 1:  Acc: 0.6545999050140381 Loss: 0.8612384796142578\n",
      "Epoch 17, CIFAR-10 Batch 2:  Acc: 0.6505998969078064 Loss: 0.8206895589828491\n",
      "Epoch 17, CIFAR-10 Batch 3:  Acc: 0.6533998847007751 Loss: 0.729544460773468\n",
      "Epoch 17, CIFAR-10 Batch 4:  Acc: 0.6627998352050781 Loss: 0.763338565826416\n",
      "Epoch 17, CIFAR-10 Batch 5:  Acc: 0.6527998447418213 Loss: 0.7605482339859009\n",
      "Epoch 18, CIFAR-10 Batch 1:  Acc: 0.6557998657226562 Loss: 0.8506314158439636\n",
      "Epoch 18, CIFAR-10 Batch 2:  Acc: 0.6477999091148376 Loss: 0.8410263061523438\n",
      "Epoch 18, CIFAR-10 Batch 3:  Acc: 0.6591998338699341 Loss: 0.7084282636642456\n",
      "Epoch 18, CIFAR-10 Batch 4:  Acc: 0.6661999225616455 Loss: 0.7409738302230835\n",
      "Epoch 18, CIFAR-10 Batch 5:  Acc: 0.6569998264312744 Loss: 0.7416462302207947\n",
      "Epoch 19, CIFAR-10 Batch 1:  Acc: 0.6479998826980591 Loss: 0.8551541566848755\n",
      "Epoch 19, CIFAR-10 Batch 2:  Acc: 0.6439998745918274 Loss: 0.8456168174743652\n",
      "Epoch 19, CIFAR-10 Batch 3:  Acc: 0.6601998805999756 Loss: 0.6902596354484558\n",
      "Epoch 19, CIFAR-10 Batch 4:  Acc: 0.660599946975708 Loss: 0.7313786745071411\n",
      "Epoch 19, CIFAR-10 Batch 5:  Acc: 0.6511998772621155 Loss: 0.731621503829956\n",
      "Epoch 20, CIFAR-10 Batch 1:  Acc: 0.6649998426437378 Loss: 0.8155912160873413\n",
      "Epoch 20, CIFAR-10 Batch 2:  Acc: 0.6587998867034912 Loss: 0.7689743638038635\n",
      "Epoch 20, CIFAR-10 Batch 3:  Acc: 0.6591998934745789 Loss: 0.6834587454795837\n",
      "Epoch 20, CIFAR-10 Batch 4:  Acc: 0.6673998832702637 Loss: 0.7147316932678223\n",
      "Epoch 20, CIFAR-10 Batch 5:  Acc: 0.6627998948097229 Loss: 0.6986665725708008\n",
      "Epoch 21, CIFAR-10 Batch 1:  Acc: 0.6719998717308044 Loss: 0.7714529037475586\n",
      "Epoch 21, CIFAR-10 Batch 2:  Acc: 0.6647999286651611 Loss: 0.7523974180221558\n",
      "Epoch 21, CIFAR-10 Batch 3:  Acc: 0.6581998467445374 Loss: 0.6642685532569885\n",
      "Epoch 21, CIFAR-10 Batch 4:  Acc: 0.6647999286651611 Loss: 0.6893409490585327\n",
      "Epoch 21, CIFAR-10 Batch 5:  Acc: 0.664199948310852 Loss: 0.6933926939964294\n",
      "Epoch 22, CIFAR-10 Batch 1:  Acc: 0.6695998311042786 Loss: 0.770095705986023\n",
      "Epoch 22, CIFAR-10 Batch 2:  Acc: 0.6619998216629028 Loss: 0.749186635017395\n",
      "Epoch 22, CIFAR-10 Batch 3:  Acc: 0.6711999177932739 Loss: 0.6373014450073242\n",
      "Epoch 22, CIFAR-10 Batch 4:  Acc: 0.6699998378753662 Loss: 0.6856833696365356\n",
      "Epoch 22, CIFAR-10 Batch 5:  Acc: 0.669999897480011 Loss: 0.6736327409744263\n",
      "Epoch 23, CIFAR-10 Batch 1:  Acc: 0.6693998575210571 Loss: 0.7779026031494141\n",
      "Epoch 23, CIFAR-10 Batch 2:  Acc: 0.6665998101234436 Loss: 0.735245406627655\n",
      "Epoch 23, CIFAR-10 Batch 3:  Acc: 0.6741998195648193 Loss: 0.6493409276008606\n",
      "Epoch 23, CIFAR-10 Batch 4:  Acc: 0.666999876499176 Loss: 0.6726826429367065\n",
      "Epoch 23, CIFAR-10 Batch 5:  Acc: 0.6753998398780823 Loss: 0.6581411361694336\n",
      "Epoch 24, CIFAR-10 Batch 1:  Acc: 0.6719998717308044 Loss: 0.7571991086006165\n",
      "Epoch 24, CIFAR-10 Batch 2:  Acc: 0.6707998514175415 Loss: 0.7596547603607178\n",
      "Epoch 24, CIFAR-10 Batch 3:  Acc: 0.6711997985839844 Loss: 0.6426133513450623\n",
      "Epoch 24, CIFAR-10 Batch 4:  Acc: 0.6757998466491699 Loss: 0.6489472389221191\n",
      "Epoch 24, CIFAR-10 Batch 5:  Acc: 0.6723999381065369 Loss: 0.6522965431213379\n",
      "Epoch 25, CIFAR-10 Batch 1:  Acc: 0.6795998215675354 Loss: 0.7336345911026001\n",
      "Epoch 25, CIFAR-10 Batch 2:  Acc: 0.6643998026847839 Loss: 0.730442225933075\n",
      "Epoch 25, CIFAR-10 Batch 3:  Acc: 0.665199875831604 Loss: 0.618083119392395\n",
      "Epoch 25, CIFAR-10 Batch 4:  Acc: 0.6807998418807983 Loss: 0.6324822902679443\n",
      "Epoch 25, CIFAR-10 Batch 5:  Acc: 0.6713998913764954 Loss: 0.6428791880607605\n",
      "Epoch 26, CIFAR-10 Batch 1:  Acc: 0.6785998940467834 Loss: 0.7120857238769531\n",
      "Epoch 26, CIFAR-10 Batch 2:  Acc: 0.6759998798370361 Loss: 0.7181347012519836\n",
      "Epoch 26, CIFAR-10 Batch 3:  Acc: 0.6763998866081238 Loss: 0.6032458543777466\n",
      "Epoch 26, CIFAR-10 Batch 4:  Acc: 0.682999849319458 Loss: 0.6294094324111938\n",
      "Epoch 26, CIFAR-10 Batch 5:  Acc: 0.6737998723983765 Loss: 0.6172604560852051\n",
      "Epoch 27, CIFAR-10 Batch 1:  Acc: 0.6831998825073242 Loss: 0.698796272277832\n",
      "Epoch 27, CIFAR-10 Batch 2:  Acc: 0.678399920463562 Loss: 0.6897468566894531\n",
      "Epoch 27, CIFAR-10 Batch 3:  Acc: 0.6793999075889587 Loss: 0.5949028730392456\n",
      "Epoch 27, CIFAR-10 Batch 4:  Acc: 0.6799998879432678 Loss: 0.6110883355140686\n",
      "Epoch 27, CIFAR-10 Batch 5:  Acc: 0.6739998459815979 Loss: 0.6040221452713013\n",
      "Epoch 28, CIFAR-10 Batch 1:  Acc: 0.6835999488830566 Loss: 0.6793056130409241\n",
      "Epoch 28, CIFAR-10 Batch 2:  Acc: 0.6855999231338501 Loss: 0.6897904872894287\n",
      "Epoch 28, CIFAR-10 Batch 3:  Acc: 0.6707998514175415 Loss: 0.6075827479362488\n",
      "Epoch 28, CIFAR-10 Batch 4:  Acc: 0.6847999095916748 Loss: 0.6052381992340088\n",
      "Epoch 28, CIFAR-10 Batch 5:  Acc: 0.6835998296737671 Loss: 0.5983839631080627\n",
      "Epoch 29, CIFAR-10 Batch 1:  Acc: 0.6855998635292053 Loss: 0.6754835247993469\n",
      "Epoch 29, CIFAR-10 Batch 2:  Acc: 0.6761999130249023 Loss: 0.6815438866615295\n",
      "Epoch 29, CIFAR-10 Batch 3:  Acc: 0.6829999089241028 Loss: 0.5586497783660889\n",
      "Epoch 29, CIFAR-10 Batch 4:  Acc: 0.686599850654602 Loss: 0.5979545712471008\n",
      "Epoch 29, CIFAR-10 Batch 5:  Acc: 0.6811999082565308 Loss: 0.592379629611969\n",
      "Epoch 30, CIFAR-10 Batch 1:  Acc: 0.681199848651886 Loss: 0.6767008304595947\n",
      "Epoch 30, CIFAR-10 Batch 2:  Acc: 0.6845998167991638 Loss: 0.6665360927581787\n",
      "Epoch 30, CIFAR-10 Batch 3:  Acc: 0.6765998005867004 Loss: 0.5790765285491943\n",
      "Epoch 30, CIFAR-10 Batch 4:  Acc: 0.6885998249053955 Loss: 0.5882636904716492\n",
      "Epoch 30, CIFAR-10 Batch 5:  Acc: 0.6885998845100403 Loss: 0.5807139873504639\n",
      "Epoch 31, CIFAR-10 Batch 1:  Acc: 0.6893998384475708 Loss: 0.6617971062660217\n",
      "Epoch 31, CIFAR-10 Batch 2:  Acc: 0.6877998113632202 Loss: 0.6575818061828613\n",
      "Epoch 31, CIFAR-10 Batch 3:  Acc: 0.6833998560905457 Loss: 0.5519174337387085\n",
      "Epoch 31, CIFAR-10 Batch 4:  Acc: 0.6959999203681946 Loss: 0.5650498867034912\n",
      "Epoch 31, CIFAR-10 Batch 5:  Acc: 0.6817998290061951 Loss: 0.5600888729095459\n",
      "Epoch 32, CIFAR-10 Batch 1:  Acc: 0.6949998736381531 Loss: 0.6326887607574463\n",
      "Epoch 32, CIFAR-10 Batch 2:  Acc: 0.6843999028205872 Loss: 0.6633485555648804\n",
      "Epoch 32, CIFAR-10 Batch 3:  Acc: 0.6873998641967773 Loss: 0.5404685139656067\n",
      "Epoch 32, CIFAR-10 Batch 4:  Acc: 0.684599757194519 Loss: 0.5559779405593872\n",
      "Epoch 32, CIFAR-10 Batch 5:  Acc: 0.6887997984886169 Loss: 0.535934329032898\n",
      "Epoch 33, CIFAR-10 Batch 1:  Acc: 0.686599850654602 Loss: 0.6478426456451416\n",
      "Epoch 33, CIFAR-10 Batch 2:  Acc: 0.6843998432159424 Loss: 0.6403388977050781\n",
      "Epoch 33, CIFAR-10 Batch 3:  Acc: 0.6839998960494995 Loss: 0.5309780836105347\n",
      "Epoch 33, CIFAR-10 Batch 4:  Acc: 0.6967998147010803 Loss: 0.5470262765884399\n",
      "Epoch 33, CIFAR-10 Batch 5:  Acc: 0.6881998777389526 Loss: 0.5375345349311829\n",
      "Epoch 34, CIFAR-10 Batch 1:  Acc: 0.6939998865127563 Loss: 0.6160491108894348\n",
      "Epoch 34, CIFAR-10 Batch 2:  Acc: 0.6919998526573181 Loss: 0.6216643452644348\n",
      "Epoch 34, CIFAR-10 Batch 3:  Acc: 0.6899998784065247 Loss: 0.5272124409675598\n",
      "Epoch 34, CIFAR-10 Batch 4:  Acc: 0.6947998404502869 Loss: 0.5370578765869141\n",
      "Epoch 34, CIFAR-10 Batch 5:  Acc: 0.6873998641967773 Loss: 0.532461404800415\n",
      "Epoch 35, CIFAR-10 Batch 1:  Acc: 0.7015998959541321 Loss: 0.5989810824394226\n",
      "Epoch 35, CIFAR-10 Batch 2:  Acc: 0.6943998336791992 Loss: 0.6155146360397339\n",
      "Epoch 35, CIFAR-10 Batch 3:  Acc: 0.6893998980522156 Loss: 0.5229493975639343\n",
      "Epoch 35, CIFAR-10 Batch 4:  Acc: 0.6983999013900757 Loss: 0.5370885133743286\n",
      "Epoch 35, CIFAR-10 Batch 5:  Acc: 0.6869999170303345 Loss: 0.5335934162139893\n",
      "Epoch 36, CIFAR-10 Batch 1:  Acc: 0.6899999380111694 Loss: 0.6243200302124023\n",
      "Epoch 36, CIFAR-10 Batch 2:  Acc: 0.6923998594284058 Loss: 0.6132400631904602\n",
      "Epoch 36, CIFAR-10 Batch 3:  Acc: 0.6957998871803284 Loss: 0.5023660063743591\n",
      "Epoch 36, CIFAR-10 Batch 4:  Acc: 0.6957998871803284 Loss: 0.5289989709854126\n",
      "Epoch 36, CIFAR-10 Batch 5:  Acc: 0.6883998513221741 Loss: 0.5316897034645081\n",
      "Epoch 37, CIFAR-10 Batch 1:  Acc: 0.6969998478889465 Loss: 0.5886597633361816\n",
      "Epoch 37, CIFAR-10 Batch 2:  Acc: 0.6967998743057251 Loss: 0.6076028943061829\n",
      "Epoch 37, CIFAR-10 Batch 3:  Acc: 0.6959998607635498 Loss: 0.51261305809021\n",
      "Epoch 37, CIFAR-10 Batch 4:  Acc: 0.6969998478889465 Loss: 0.5227651596069336\n",
      "Epoch 37, CIFAR-10 Batch 5:  Acc: 0.689599871635437 Loss: 0.5106371641159058\n",
      "Epoch 38, CIFAR-10 Batch 1:  Acc: 0.6963998079299927 Loss: 0.6042265892028809\n",
      "Epoch 38, CIFAR-10 Batch 2:  Acc: 0.6953998804092407 Loss: 0.5958796143531799\n",
      "Epoch 38, CIFAR-10 Batch 3:  Acc: 0.6891998648643494 Loss: 0.517090916633606\n",
      "Epoch 38, CIFAR-10 Batch 4:  Acc: 0.7011998891830444 Loss: 0.5101186037063599\n",
      "Epoch 38, CIFAR-10 Batch 5:  Acc: 0.6937998533248901 Loss: 0.5078257918357849\n",
      "Epoch 39, CIFAR-10 Batch 1:  Acc: 0.6953998804092407 Loss: 0.5877919793128967\n",
      "Epoch 39, CIFAR-10 Batch 2:  Acc: 0.6967998743057251 Loss: 0.5883544683456421\n",
      "Epoch 39, CIFAR-10 Batch 3:  Acc: 0.6871998310089111 Loss: 0.5002595782279968\n",
      "Epoch 39, CIFAR-10 Batch 4:  Acc: 0.701999843120575 Loss: 0.513970136642456\n",
      "Epoch 39, CIFAR-10 Batch 5:  Acc: 0.6957999467849731 Loss: 0.5020233392715454\n",
      "Epoch 40, CIFAR-10 Batch 1:  Acc: 0.7049998641014099 Loss: 0.5652244687080383\n",
      "Epoch 40, CIFAR-10 Batch 2:  Acc: 0.6975998878479004 Loss: 0.5651987195014954\n",
      "Epoch 40, CIFAR-10 Batch 3:  Acc: 0.6927998661994934 Loss: 0.4978020191192627\n",
      "Epoch 40, CIFAR-10 Batch 4:  Acc: 0.7035998702049255 Loss: 0.5029235482215881\n",
      "Epoch 40, CIFAR-10 Batch 5:  Acc: 0.7009998559951782 Loss: 0.47442626953125\n",
      "Epoch 41, CIFAR-10 Batch 1:  Acc: 0.6961998343467712 Loss: 0.5607460141181946\n",
      "Epoch 41, CIFAR-10 Batch 2:  Acc: 0.6861999034881592 Loss: 0.5646108984947205\n",
      "Epoch 41, CIFAR-10 Batch 3:  Acc: 0.6837998628616333 Loss: 0.4924292266368866\n",
      "Epoch 41, CIFAR-10 Batch 4:  Acc: 0.7005998492240906 Loss: 0.5019193887710571\n",
      "Epoch 41, CIFAR-10 Batch 5:  Acc: 0.6901999115943909 Loss: 0.48550230264663696\n",
      "Epoch 42, CIFAR-10 Batch 1:  Acc: 0.6979998350143433 Loss: 0.5656691193580627\n",
      "Epoch 42, CIFAR-10 Batch 2:  Acc: 0.6819998621940613 Loss: 0.5644468069076538\n",
      "Epoch 42, CIFAR-10 Batch 3:  Acc: 0.6851998567581177 Loss: 0.4982820153236389\n",
      "Epoch 42, CIFAR-10 Batch 4:  Acc: 0.6999998092651367 Loss: 0.5256360769271851\n",
      "Epoch 42, CIFAR-10 Batch 5:  Acc: 0.6999998092651367 Loss: 0.48925426602363586\n",
      "Epoch 43, CIFAR-10 Batch 1:  Acc: 0.699199914932251 Loss: 0.5546533465385437\n",
      "Epoch 43, CIFAR-10 Batch 2:  Acc: 0.6919998526573181 Loss: 0.5485380291938782\n",
      "Epoch 43, CIFAR-10 Batch 3:  Acc: 0.6947998404502869 Loss: 0.4585837125778198\n",
      "Epoch 43, CIFAR-10 Batch 4:  Acc: 0.6975998878479004 Loss: 0.4868130087852478\n",
      "Epoch 43, CIFAR-10 Batch 5:  Acc: 0.6999998092651367 Loss: 0.4657435417175293\n",
      "Epoch 44, CIFAR-10 Batch 1:  Acc: 0.7023999094963074 Loss: 0.5361549854278564\n",
      "Epoch 44, CIFAR-10 Batch 2:  Acc: 0.7097998857498169 Loss: 0.5308551788330078\n",
      "Epoch 44, CIFAR-10 Batch 3:  Acc: 0.6805999279022217 Loss: 0.4699957072734833\n",
      "Epoch 44, CIFAR-10 Batch 4:  Acc: 0.7029998302459717 Loss: 0.4938697814941406\n",
      "Epoch 44, CIFAR-10 Batch 5:  Acc: 0.7029998898506165 Loss: 0.46430346369743347\n",
      "Epoch 45, CIFAR-10 Batch 1:  Acc: 0.7021999359130859 Loss: 0.5280871391296387\n",
      "Epoch 45, CIFAR-10 Batch 2:  Acc: 0.6957998275756836 Loss: 0.5362437963485718\n",
      "Epoch 45, CIFAR-10 Batch 3:  Acc: 0.6887998580932617 Loss: 0.4781653881072998\n",
      "Epoch 45, CIFAR-10 Batch 4:  Acc: 0.712199866771698 Loss: 0.4722849428653717\n",
      "Epoch 45, CIFAR-10 Batch 5:  Acc: 0.6917998790740967 Loss: 0.4420734941959381\n",
      "Epoch 46, CIFAR-10 Batch 1:  Acc: 0.7001998424530029 Loss: 0.5297003984451294\n",
      "Epoch 46, CIFAR-10 Batch 2:  Acc: 0.7043999433517456 Loss: 0.5067304372787476\n",
      "Epoch 46, CIFAR-10 Batch 3:  Acc: 0.6965998411178589 Loss: 0.44946572184562683\n",
      "Epoch 46, CIFAR-10 Batch 4:  Acc: 0.7065998315811157 Loss: 0.4702773988246918\n",
      "Epoch 46, CIFAR-10 Batch 5:  Acc: 0.7015998959541321 Loss: 0.46753549575805664\n",
      "Epoch 47, CIFAR-10 Batch 1:  Acc: 0.7087998390197754 Loss: 0.509699285030365\n",
      "Epoch 47, CIFAR-10 Batch 2:  Acc: 0.704399824142456 Loss: 0.49784016609191895\n",
      "Epoch 47, CIFAR-10 Batch 3:  Acc: 0.6959998607635498 Loss: 0.44523558020591736\n",
      "Epoch 47, CIFAR-10 Batch 4:  Acc: 0.7043998837471008 Loss: 0.4564356803894043\n",
      "Epoch 47, CIFAR-10 Batch 5:  Acc: 0.6973998546600342 Loss: 0.4407595098018646\n",
      "Epoch 48, CIFAR-10 Batch 1:  Acc: 0.7069999575614929 Loss: 0.5060122609138489\n",
      "Epoch 48, CIFAR-10 Batch 2:  Acc: 0.705599844455719 Loss: 0.4859088063240051\n",
      "Epoch 48, CIFAR-10 Batch 3:  Acc: 0.6893998384475708 Loss: 0.43442803621292114\n",
      "Epoch 48, CIFAR-10 Batch 4:  Acc: 0.7033998966217041 Loss: 0.44312942028045654\n",
      "Epoch 48, CIFAR-10 Batch 5:  Acc: 0.703799843788147 Loss: 0.43368199467658997\n",
      "Epoch 49, CIFAR-10 Batch 1:  Acc: 0.7093998789787292 Loss: 0.4895519018173218\n",
      "Epoch 49, CIFAR-10 Batch 2:  Acc: 0.705599844455719 Loss: 0.47151216864585876\n",
      "Epoch 49, CIFAR-10 Batch 3:  Acc: 0.6911998391151428 Loss: 0.43499359488487244\n",
      "Epoch 49, CIFAR-10 Batch 4:  Acc: 0.705599844455719 Loss: 0.4299301505088806\n",
      "Epoch 49, CIFAR-10 Batch 5:  Acc: 0.6997998952865601 Loss: 0.4276633858680725\n",
      "Epoch 50, CIFAR-10 Batch 1:  Acc: 0.7057998776435852 Loss: 0.4963178038597107\n",
      "Epoch 50, CIFAR-10 Batch 2:  Acc: 0.7059999108314514 Loss: 0.47659409046173096\n",
      "Epoch 50, CIFAR-10 Batch 3:  Acc: 0.6985998153686523 Loss: 0.4071740210056305\n",
      "Epoch 50, CIFAR-10 Batch 4:  Acc: 0.7055997848510742 Loss: 0.4184207320213318\n",
      "Epoch 50, CIFAR-10 Batch 5:  Acc: 0.7041998505592346 Loss: 0.39471936225891113\n",
      "Epoch 51, CIFAR-10 Batch 1:  Acc: 0.7097998857498169 Loss: 0.4885687530040741\n",
      "Epoch 51, CIFAR-10 Batch 2:  Acc: 0.709199845790863 Loss: 0.4538951814174652\n",
      "Epoch 51, CIFAR-10 Batch 3:  Acc: 0.697399914264679 Loss: 0.4197547733783722\n",
      "Epoch 51, CIFAR-10 Batch 4:  Acc: 0.7069998383522034 Loss: 0.43717965483665466\n",
      "Epoch 51, CIFAR-10 Batch 5:  Acc: 0.7051998376846313 Loss: 0.418944776058197\n",
      "Epoch 52, CIFAR-10 Batch 1:  Acc: 0.7069997787475586 Loss: 0.47101283073425293\n",
      "Epoch 52, CIFAR-10 Batch 2:  Acc: 0.7079998254776001 Loss: 0.46886253356933594\n",
      "Epoch 52, CIFAR-10 Batch 3:  Acc: 0.692599892616272 Loss: 0.43039220571517944\n",
      "Epoch 52, CIFAR-10 Batch 4:  Acc: 0.7123997807502747 Loss: 0.4154602289199829\n",
      "Epoch 52, CIFAR-10 Batch 5:  Acc: 0.7061998248100281 Loss: 0.40035268664360046\n",
      "Epoch 53, CIFAR-10 Batch 1:  Acc: 0.6979998350143433 Loss: 0.48613253235816956\n",
      "Epoch 53, CIFAR-10 Batch 2:  Acc: 0.7059998512268066 Loss: 0.44557812809944153\n",
      "Epoch 53, CIFAR-10 Batch 3:  Acc: 0.6963999271392822 Loss: 0.40674635767936707\n",
      "Epoch 53, CIFAR-10 Batch 4:  Acc: 0.7081997990608215 Loss: 0.4044840335845947\n",
      "Epoch 53, CIFAR-10 Batch 5:  Acc: 0.7071998715400696 Loss: 0.39406973123550415\n",
      "Epoch 54, CIFAR-10 Batch 1:  Acc: 0.7049999237060547 Loss: 0.4795662462711334\n",
      "Epoch 54, CIFAR-10 Batch 2:  Acc: 0.6963999271392822 Loss: 0.473911851644516\n",
      "Epoch 54, CIFAR-10 Batch 3:  Acc: 0.6935999393463135 Loss: 0.4169144034385681\n",
      "Epoch 54, CIFAR-10 Batch 4:  Acc: 0.7117998600006104 Loss: 0.40054789185523987\n",
      "Epoch 54, CIFAR-10 Batch 5:  Acc: 0.7079998850822449 Loss: 0.39753231406211853\n",
      "Epoch 55, CIFAR-10 Batch 1:  Acc: 0.7047998905181885 Loss: 0.46048200130462646\n",
      "Epoch 55, CIFAR-10 Batch 2:  Acc: 0.7099998593330383 Loss: 0.439397931098938\n",
      "Epoch 55, CIFAR-10 Batch 3:  Acc: 0.7095998525619507 Loss: 0.39026594161987305\n",
      "Epoch 55, CIFAR-10 Batch 4:  Acc: 0.6995998620986938 Loss: 0.4126507043838501\n",
      "Epoch 55, CIFAR-10 Batch 5:  Acc: 0.7087998986244202 Loss: 0.37659919261932373\n",
      "Epoch 56, CIFAR-10 Batch 1:  Acc: 0.7029998898506165 Loss: 0.43292665481567383\n",
      "Epoch 56, CIFAR-10 Batch 2:  Acc: 0.7127997875213623 Loss: 0.4255537986755371\n",
      "Epoch 56, CIFAR-10 Batch 3:  Acc: 0.6941999197006226 Loss: 0.39432013034820557\n",
      "Epoch 56, CIFAR-10 Batch 4:  Acc: 0.7123998999595642 Loss: 0.40565380454063416\n",
      "Epoch 56, CIFAR-10 Batch 5:  Acc: 0.7037999033927917 Loss: 0.41064268350601196\n",
      "Epoch 57, CIFAR-10 Batch 1:  Acc: 0.7047998905181885 Loss: 0.4574573040008545\n",
      "Epoch 57, CIFAR-10 Batch 2:  Acc: 0.712199866771698 Loss: 0.44580236077308655\n",
      "Epoch 57, CIFAR-10 Batch 3:  Acc: 0.7019999027252197 Loss: 0.3679203987121582\n",
      "Epoch 57, CIFAR-10 Batch 4:  Acc: 0.710399866104126 Loss: 0.39970287680625916\n",
      "Epoch 57, CIFAR-10 Batch 5:  Acc: 0.703799843788147 Loss: 0.3749535381793976\n",
      "Epoch 58, CIFAR-10 Batch 1:  Acc: 0.7129998207092285 Loss: 0.4431809186935425\n",
      "Epoch 58, CIFAR-10 Batch 2:  Acc: 0.7071998119354248 Loss: 0.41719168424606323\n",
      "Epoch 58, CIFAR-10 Batch 3:  Acc: 0.6869999170303345 Loss: 0.38525480031967163\n",
      "Epoch 58, CIFAR-10 Batch 4:  Acc: 0.6979998350143433 Loss: 0.40247049927711487\n",
      "Epoch 58, CIFAR-10 Batch 5:  Acc: 0.7063997983932495 Loss: 0.3781849443912506\n",
      "Epoch 59, CIFAR-10 Batch 1:  Acc: 0.7083998322486877 Loss: 0.4330177307128906\n",
      "Epoch 59, CIFAR-10 Batch 2:  Acc: 0.7057998776435852 Loss: 0.4211168885231018\n",
      "Epoch 59, CIFAR-10 Batch 3:  Acc: 0.7073999047279358 Loss: 0.37461912631988525\n",
      "Epoch 59, CIFAR-10 Batch 4:  Acc: 0.7087998390197754 Loss: 0.37888965010643005\n",
      "Epoch 59, CIFAR-10 Batch 5:  Acc: 0.7129998803138733 Loss: 0.3620792031288147\n",
      "Epoch 60, CIFAR-10 Batch 1:  Acc: 0.7043998837471008 Loss: 0.4474979341030121\n",
      "Epoch 60, CIFAR-10 Batch 2:  Acc: 0.6997998356819153 Loss: 0.421286404132843\n",
      "Epoch 60, CIFAR-10 Batch 3:  Acc: 0.7035998702049255 Loss: 0.3763902187347412\n",
      "Epoch 60, CIFAR-10 Batch 4:  Acc: 0.7095999121665955 Loss: 0.3928171992301941\n",
      "Epoch 60, CIFAR-10 Batch 5:  Acc: 0.7147998213768005 Loss: 0.3586356043815613\n",
      "Epoch 61, CIFAR-10 Batch 1:  Acc: 0.7179998755455017 Loss: 0.4150947630405426\n",
      "Epoch 61, CIFAR-10 Batch 2:  Acc: 0.7051998972892761 Loss: 0.40325823426246643\n",
      "Epoch 61, CIFAR-10 Batch 3:  Acc: 0.6889999508857727 Loss: 0.3625335693359375\n",
      "Epoch 61, CIFAR-10 Batch 4:  Acc: 0.7099998593330383 Loss: 0.3673391044139862\n",
      "Epoch 61, CIFAR-10 Batch 5:  Acc: 0.7095997929573059 Loss: 0.35773009061813354\n",
      "Epoch 62, CIFAR-10 Batch 1:  Acc: 0.7109998464584351 Loss: 0.41284751892089844\n",
      "Epoch 62, CIFAR-10 Batch 2:  Acc: 0.7107998728752136 Loss: 0.40330612659454346\n",
      "Epoch 62, CIFAR-10 Batch 3:  Acc: 0.6945998668670654 Loss: 0.3836485743522644\n",
      "Epoch 62, CIFAR-10 Batch 4:  Acc: 0.6903998851776123 Loss: 0.41302451491355896\n",
      "Epoch 62, CIFAR-10 Batch 5:  Acc: 0.7061998844146729 Loss: 0.3504703938961029\n",
      "Epoch 63, CIFAR-10 Batch 1:  Acc: 0.7115998268127441 Loss: 0.4166758358478546\n",
      "Epoch 63, CIFAR-10 Batch 2:  Acc: 0.7089998126029968 Loss: 0.3947864770889282\n",
      "Epoch 63, CIFAR-10 Batch 3:  Acc: 0.7089999318122864 Loss: 0.35638803243637085\n",
      "Epoch 63, CIFAR-10 Batch 4:  Acc: 0.7025998830795288 Loss: 0.3715118169784546\n",
      "Epoch 63, CIFAR-10 Batch 5:  Acc: 0.7003998756408691 Loss: 0.3458467125892639\n",
      "Epoch 64, CIFAR-10 Batch 1:  Acc: 0.7037997841835022 Loss: 0.4150097966194153\n",
      "Epoch 64, CIFAR-10 Batch 2:  Acc: 0.7125998735427856 Loss: 0.38930413126945496\n",
      "Epoch 64, CIFAR-10 Batch 3:  Acc: 0.7081998586654663 Loss: 0.35106992721557617\n",
      "Epoch 64, CIFAR-10 Batch 4:  Acc: 0.7143998146057129 Loss: 0.3583250641822815\n",
      "Epoch 64, CIFAR-10 Batch 5:  Acc: 0.7085998058319092 Loss: 0.3411683440208435\n",
      "Epoch 65, CIFAR-10 Batch 1:  Acc: 0.7123998403549194 Loss: 0.3947315812110901\n",
      "Epoch 65, CIFAR-10 Batch 2:  Acc: 0.7123998999595642 Loss: 0.3941989541053772\n",
      "Epoch 65, CIFAR-10 Batch 3:  Acc: 0.7091999053955078 Loss: 0.35381776094436646\n",
      "Epoch 65, CIFAR-10 Batch 4:  Acc: 0.7107998728752136 Loss: 0.36315861344337463\n",
      "Epoch 65, CIFAR-10 Batch 5:  Acc: 0.7087998390197754 Loss: 0.3306802213191986\n",
      "Epoch 66, CIFAR-10 Batch 1:  Acc: 0.710399866104126 Loss: 0.4031592309474945\n",
      "Epoch 66, CIFAR-10 Batch 2:  Acc: 0.7149999141693115 Loss: 0.3830147087574005\n",
      "Epoch 66, CIFAR-10 Batch 3:  Acc: 0.7055999040603638 Loss: 0.35454079508781433\n",
      "Epoch 66, CIFAR-10 Batch 4:  Acc: 0.7069998383522034 Loss: 0.3575868308544159\n",
      "Epoch 66, CIFAR-10 Batch 5:  Acc: 0.7093998193740845 Loss: 0.3338938057422638\n",
      "Epoch 67, CIFAR-10 Batch 1:  Acc: 0.7105998992919922 Loss: 0.38904982805252075\n",
      "Epoch 67, CIFAR-10 Batch 2:  Acc: 0.7055999040603638 Loss: 0.37896960973739624\n",
      "Epoch 67, CIFAR-10 Batch 3:  Acc: 0.7089998722076416 Loss: 0.35174205899238586\n",
      "Epoch 67, CIFAR-10 Batch 4:  Acc: 0.7087998390197754 Loss: 0.3529598116874695\n",
      "Epoch 67, CIFAR-10 Batch 5:  Acc: 0.7047998309135437 Loss: 0.32722726464271545\n",
      "Epoch 68, CIFAR-10 Batch 1:  Acc: 0.7151998281478882 Loss: 0.3783468008041382\n",
      "Epoch 68, CIFAR-10 Batch 2:  Acc: 0.709199845790863 Loss: 0.3700273633003235\n",
      "Epoch 68, CIFAR-10 Batch 3:  Acc: 0.7059999108314514 Loss: 0.33363077044487\n",
      "Epoch 68, CIFAR-10 Batch 4:  Acc: 0.7041999101638794 Loss: 0.35306984186172485\n",
      "Epoch 68, CIFAR-10 Batch 5:  Acc: 0.7135998606681824 Loss: 0.3186187744140625\n",
      "Epoch 69, CIFAR-10 Batch 1:  Acc: 0.7153998613357544 Loss: 0.37178266048431396\n",
      "Epoch 69, CIFAR-10 Batch 2:  Acc: 0.7149998545646667 Loss: 0.368953675031662\n",
      "Epoch 69, CIFAR-10 Batch 3:  Acc: 0.7033998966217041 Loss: 0.3759543299674988\n",
      "Epoch 69, CIFAR-10 Batch 4:  Acc: 0.7169998288154602 Loss: 0.34605008363723755\n",
      "Epoch 69, CIFAR-10 Batch 5:  Acc: 0.7101998329162598 Loss: 0.34041234850883484\n",
      "Epoch 70, CIFAR-10 Batch 1:  Acc: 0.7143998146057129 Loss: 0.40283891558647156\n",
      "Epoch 70, CIFAR-10 Batch 2:  Acc: 0.7103998064994812 Loss: 0.36234718561172485\n",
      "Epoch 70, CIFAR-10 Batch 3:  Acc: 0.7083998918533325 Loss: 0.3374750316143036\n",
      "Epoch 70, CIFAR-10 Batch 4:  Acc: 0.7009997963905334 Loss: 0.34857603907585144\n",
      "Epoch 70, CIFAR-10 Batch 5:  Acc: 0.7127999067306519 Loss: 0.3271303176879883\n",
      "Epoch 71, CIFAR-10 Batch 1:  Acc: 0.7105998992919922 Loss: 0.3762771189212799\n",
      "Epoch 71, CIFAR-10 Batch 2:  Acc: 0.715199887752533 Loss: 0.3575713634490967\n",
      "Epoch 71, CIFAR-10 Batch 3:  Acc: 0.7067998647689819 Loss: 0.34344202280044556\n",
      "Epoch 71, CIFAR-10 Batch 4:  Acc: 0.7135998010635376 Loss: 0.3257976174354553\n",
      "Epoch 71, CIFAR-10 Batch 5:  Acc: 0.708599865436554 Loss: 0.32457929849624634\n",
      "Epoch 72, CIFAR-10 Batch 1:  Acc: 0.7117998600006104 Loss: 0.3621031939983368\n",
      "Epoch 72, CIFAR-10 Batch 2:  Acc: 0.7193998098373413 Loss: 0.35306036472320557\n",
      "Epoch 72, CIFAR-10 Batch 3:  Acc: 0.7047997713088989 Loss: 0.3360639214515686\n",
      "Epoch 72, CIFAR-10 Batch 4:  Acc: 0.7065998315811157 Loss: 0.34754297137260437\n",
      "Epoch 72, CIFAR-10 Batch 5:  Acc: 0.7071998119354248 Loss: 0.328592449426651\n",
      "Epoch 73, CIFAR-10 Batch 1:  Acc: 0.7131998538970947 Loss: 0.350019633769989\n",
      "Epoch 73, CIFAR-10 Batch 2:  Acc: 0.7111998796463013 Loss: 0.3487444221973419\n",
      "Epoch 73, CIFAR-10 Batch 3:  Acc: 0.7059998512268066 Loss: 0.33505910634994507\n",
      "Epoch 73, CIFAR-10 Batch 4:  Acc: 0.7077998518943787 Loss: 0.33029109239578247\n",
      "Epoch 73, CIFAR-10 Batch 5:  Acc: 0.707399845123291 Loss: 0.31690481305122375\n",
      "Epoch 74, CIFAR-10 Batch 1:  Acc: 0.709199845790863 Loss: 0.35659241676330566\n",
      "Epoch 74, CIFAR-10 Batch 2:  Acc: 0.7163998484611511 Loss: 0.35584399104118347\n",
      "Epoch 74, CIFAR-10 Batch 3:  Acc: 0.7057998776435852 Loss: 0.32217901945114136\n",
      "Epoch 74, CIFAR-10 Batch 4:  Acc: 0.7149998545646667 Loss: 0.3192402422428131\n",
      "Epoch 74, CIFAR-10 Batch 5:  Acc: 0.7113998532295227 Loss: 0.2914791703224182\n",
      "Epoch 75, CIFAR-10 Batch 1:  Acc: 0.7155998349189758 Loss: 0.37381890416145325\n",
      "Epoch 75, CIFAR-10 Batch 2:  Acc: 0.7047998905181885 Loss: 0.333723247051239\n",
      "Epoch 75, CIFAR-10 Batch 3:  Acc: 0.7069998979568481 Loss: 0.29982325434684753\n",
      "Epoch 75, CIFAR-10 Batch 4:  Acc: 0.7127998471260071 Loss: 0.3242560029029846\n",
      "Epoch 75, CIFAR-10 Batch 5:  Acc: 0.7071998119354248 Loss: 0.2993510067462921\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7018841952085495\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxCYHBZ1cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f6e99c3b00>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Request to Reviewer: \n",
    "Ideas on better networks, better development practices and visualization relevant to this assignment would be really helpful\n",
    "\n",
    "Note to the Reviewer: The hyperparameters and architecture design are inspired from [this link](www.alivelearn.net/deeplearning/dlnd_image_classification_submission2.html) which is the work of a fellow Udacian it seems. \n",
    "I felt bad borrowing ideas from there without crediting it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
